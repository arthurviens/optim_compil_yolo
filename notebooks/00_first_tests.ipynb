{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=\"$TVM_HOME/python\"\n",
    "# !echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model with the relax frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend import nn\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.softmax = nn.softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), fc1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), fc1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), fc2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), fc2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            permute_dims: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(fc1_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            matmul: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, permute_dims, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            add: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(matmul, fc1_bias)\n",
       "            relu: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(add)\n",
       "            permute_dims1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(fc2_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            matmul1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(relu, permute_dims1, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            add1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(matmul1, fc2_bias)\n",
       "            softmax: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>softmax(add1, axis<span style=\"color: #A2F; font-weight: bold\">=-</span><span style=\"color: #008000\">1</span>)\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> softmax\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod, param_spec = MLPModel().export_tvm(\n",
    "    spec={\"forward\": {\"x\": nn.spec.Tensor((1, 784), \"float32\")}}\n",
    ")\n",
    "mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower relax to TensorIR\n",
    "In the \"zero\" pipeline, 5 passes are applied:\n",
    "- LegalizeOps() : \"Legalize high-level operator calls in Relax functions to call_tir with corresponding low-level TIR PrimFuncs.\"\n",
    "- AnnotateTIROpPattern() : \"Annotate Op Pattern Kind for TIR functions.\"\n",
    "- FoldConstant() : \"Fold constant expressions within dataflow blocks.\"\n",
    "- FuseOps() : This pass groups bindings in a dataflow block of Relax functions and generate a new grouped Relax function for each group, according to the fusion algorithm described in the pass implementation. By grouping bindings into new Relax functions, we substitute the bindings in the function being manipulated into function calls to the new grouped function. A follow-up pass named \"FuseTIR\" will generate a TIR PrimFunc for each grouped function.\n",
    "- FuseTIR() : Fuse primitive relax function into a larger TIR function if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# from tvm.script import ir as I\n",
       "# from tvm.script import tir as T\n",
       "# from tvm.script import relax as R\n",
       "\n",
       "@I.ir_module\n",
       "class Module:\n",
       "    @T.prim_func(private=True)\n",
       "    def fused_matmul1_add1(relu: T.Buffer((T.int64(1), T.int64(256)), \"float32\"), permute_dims1: T.Buffer((T.int64(256), T.int64(10)), \"float32\"), fc2_bias: T.Buffer((T.int64(10),), \"float32\"), T_add_intermediate: T.Buffer((T.int64(1), T.int64(10)), \"float32\")):\n",
       "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(10)))\n",
       "        for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(256)):\n",
       "            with T.block(\"matmul\"):\n",
       "                v_i0, v_i1, v_k = T.axis.remap(\"SSR\", [i0, i1, k])\n",
       "                T.reads(relu[v_i0, v_k], permute_dims1[v_k, v_i1])\n",
       "                T.writes(matmul_intermediate[v_i0, v_i1])\n",
       "                with T.init():\n",
       "                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)\n",
       "                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + relu[v_i0, v_k] * permute_dims1[v_k, v_i1]\n",
       "        for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):\n",
       "            with T.block(\"T_add\"):\n",
       "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
       "                T.reads(matmul_intermediate[v_ax0, v_ax1], fc2_bias[v_ax1])\n",
       "                T.writes(T_add_intermediate[v_ax0, v_ax1])\n",
       "                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + fc2_bias[v_ax1]\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def fused_matmul_add_relu(x: T.Buffer((T.int64(1), T.int64(784)), \"float32\"), permute_dims: T.Buffer((T.int64(784), T.int64(256)), \"float32\"), fc1_bias: T.Buffer((T.int64(256),), \"float32\"), compute_intermediate: T.Buffer((T.int64(1), T.int64(256)), \"float32\")):\n",
       "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))\n",
       "        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))\n",
       "        for i0, i1, k in T.grid(T.int64(1), T.int64(256), T.int64(784)):\n",
       "            with T.block(\"matmul\"):\n",
       "                v_i0, v_i1, v_k = T.axis.remap(\"SSR\", [i0, i1, k])\n",
       "                T.reads(x[v_i0, v_k], permute_dims[v_k, v_i1])\n",
       "                T.writes(matmul_intermediate[v_i0, v_i1])\n",
       "                with T.init():\n",
       "                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)\n",
       "                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + x[v_i0, v_k] * permute_dims[v_k, v_i1]\n",
       "        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):\n",
       "            with T.block(\"T_add\"):\n",
       "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
       "                T.reads(matmul_intermediate[v_ax0, v_ax1], fc1_bias[v_ax1])\n",
       "                T.writes(T_add_intermediate[v_ax0, v_ax1])\n",
       "                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + fc1_bias[v_ax1]\n",
       "        for i0, i1 in T.grid(T.int64(1), T.int64(256)):\n",
       "            with T.block(\"compute\"):\n",
       "                v_i0, v_i1 = T.axis.remap(\"SS\", [i0, i1])\n",
       "                T.reads(T_add_intermediate[v_i0, v_i1])\n",
       "                T.writes(compute_intermediate[v_i0, v_i1])\n",
       "                compute_intermediate[v_i0, v_i1] = T.max(T_add_intermediate[v_i0, v_i1], T.float32(0.0))\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def softmax(add1: T.Buffer((T.int64(1), T.int64(10)), \"float32\"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(10)), \"float32\")):\n",
       "        T.func_attr({\"op_pattern\": 4, \"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        T_softmax_maxelem = T.alloc_buffer((T.int64(1),))\n",
       "        T_softmax_exp = T.alloc_buffer((T.int64(1), T.int64(10)))\n",
       "        T_softmax_expsum = T.alloc_buffer((T.int64(1),))\n",
       "        for i0, k in T.grid(T.int64(1), T.int64(10)):\n",
       "            with T.block(\"T_softmax_maxelem\"):\n",
       "                v_i0, v_k = T.axis.remap(\"SR\", [i0, k])\n",
       "                T.reads(add1[v_i0, v_k])\n",
       "                T.writes(T_softmax_maxelem[v_i0])\n",
       "                with T.init():\n",
       "                    T_softmax_maxelem[v_i0] = T.float32(-340282346638528859811704183484516925440.0)\n",
       "                T_softmax_maxelem[v_i0] = T.max(T_softmax_maxelem[v_i0], add1[v_i0, v_k])\n",
       "        for i0, i1 in T.grid(T.int64(1), T.int64(10)):\n",
       "            with T.block(\"T_softmax_exp\"):\n",
       "                v_i0, v_i1 = T.axis.remap(\"SS\", [i0, i1])\n",
       "                T.reads(add1[v_i0, v_i1], T_softmax_maxelem[v_i0])\n",
       "                T.writes(T_softmax_exp[v_i0, v_i1])\n",
       "                T_softmax_exp[v_i0, v_i1] = T.exp(add1[v_i0, v_i1] - T_softmax_maxelem[v_i0])\n",
       "        for i0, k in T.grid(T.int64(1), T.int64(10)):\n",
       "            with T.block(\"T_softmax_expsum\"):\n",
       "                v_i0, v_k = T.axis.remap(\"SR\", [i0, k])\n",
       "                T.reads(T_softmax_exp[v_i0, v_k])\n",
       "                T.writes(T_softmax_expsum[v_i0])\n",
       "                with T.init():\n",
       "                    T_softmax_expsum[v_i0] = T.float32(0.0)\n",
       "                T_softmax_expsum[v_i0] = T_softmax_expsum[v_i0] + T_softmax_exp[v_i0, v_k]\n",
       "        for i0, i1 in T.grid(T.int64(1), T.int64(10)):\n",
       "            with T.block(\"T_softmax_norm\"):\n",
       "                v_i0, v_i1 = T.axis.remap(\"SS\", [i0, i1])\n",
       "                T.reads(T_softmax_exp[v_i0, v_i1], T_softmax_expsum[v_i0])\n",
       "                T.writes(T_softmax_norm[v_i0, v_i1])\n",
       "                T.block_attr({\"axis\": 1})\n",
       "                T_softmax_norm[v_i0, v_i1] = T_softmax_exp[v_i0, v_i1] / T_softmax_expsum[v_i0]\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def transpose(fc1_weight: T.Buffer((T.int64(256), T.int64(784)), \"float32\"), T_transpose: T.Buffer((T.int64(784), T.int64(256)), \"float32\")):\n",
       "        T.func_attr({\"op_pattern\": 2, \"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        for ax0, ax1 in T.grid(T.int64(784), T.int64(256)):\n",
       "            with T.block(\"T_transpose\"):\n",
       "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
       "                T.reads(fc1_weight[v_ax1, v_ax0])\n",
       "                T.writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] = fc1_weight[v_ax1, v_ax0]\n",
       "\n",
       "    @T.prim_func(private=True)\n",
       "    def transpose1(fc2_weight: T.Buffer((T.int64(10), T.int64(256)), \"float32\"), T_transpose: T.Buffer((T.int64(256), T.int64(10)), \"float32\")):\n",
       "        T.func_attr({\"op_pattern\": 2, \"tir.noalias\": T.bool(True)})\n",
       "        # with T.block(\"root\"):\n",
       "        for ax0, ax1 in T.grid(T.int64(256), T.int64(10)):\n",
       "            with T.block(\"T_transpose\"):\n",
       "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
       "                T.reads(fc2_weight[v_ax1, v_ax0])\n",
       "                T.writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] = fc2_weight[v_ax1, v_ax0]\n",
       "\n",
       "    @R.function\n",
       "    def forward(x: R.Tensor((1, 784), dtype=\"float32\"), fc1_weight: R.Tensor((256, 784), dtype=\"float32\"), fc1_bias: R.Tensor((256,), dtype=\"float32\"), fc2_weight: R.Tensor((10, 256), dtype=\"float32\"), fc2_bias: R.Tensor((10,), dtype=\"float32\")) -> R.Tensor((1, 10), dtype=\"float32\"):\n",
       "        R.func_attr({\"num_input\": 1})\n",
       "        cls = Module\n",
       "        with R.dataflow():\n",
       "            permute_dims = R.call_tir(cls.transpose, (fc1_weight,), out_sinfo=R.Tensor((784, 256), dtype=\"float32\"))\n",
       "            lv = R.call_tir(cls.fused_matmul_add_relu, (x, permute_dims, fc1_bias), out_sinfo=R.Tensor((1, 256), dtype=\"float32\"))\n",
       "            permute_dims1 = R.call_tir(cls.transpose1, (fc2_weight,), out_sinfo=R.Tensor((256, 10), dtype=\"float32\"))\n",
       "            lv1 = R.call_tir(cls.fused_matmul1_add1, (lv, permute_dims1, fc2_bias), out_sinfo=R.Tensor((1, 10), dtype=\"float32\"))\n",
       "            gv = R.call_tir(cls.softmax, (lv1,), out_sinfo=R.Tensor((1, 10), dtype=\"float32\"))\n",
       "            R.output(gv)\n",
       "        return gv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe0 = relax.get_pipeline(\"zero\")(mod)\n",
    "pipe0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = tvm.target.Target(\"llvm\")\n",
    "ex = relax.build(mod, target)\n",
    "device = tvm.cpu()\n",
    "vm = relax.VirtualMachine(ex, device)\n",
    "data = np.random.rand(1, 784).astype(\"float32\")\n",
    "tvm_data = tvm.nd.array(data, device=device)\n",
    "params = [np.random.rand(*param.shape).astype(\"float32\") for _, param in param_spec]\n",
    "params = [tvm.nd.array(param, device=device) for param in params]\n",
    "print(vm[\"forward\"](tvm_data, *params).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
