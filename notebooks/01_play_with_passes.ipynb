{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(x, conv1_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(conv1_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
       "            conv2d: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, lv1)\n",
       "            relu: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(conv2d)\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(relu, conv2_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(conv2_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
       "            conv2d1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv2, lv3)\n",
       "            relu1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(conv2d1)\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> relu1\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import typing\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm import relax\n",
    "import tvm.contrib.graph_executor as runtime\n",
    "import numpy as np\n",
    "\n",
    "class MyRelaxModel(relax.frontend.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRelaxModel, self).__init__()\n",
    "        self.conv1 = relax.frontend.nn.Conv2D(3, 32, kernel_size=5, stride=2, padding=2, bias=True)\n",
    "        self.relu1 = relax.frontend.nn.ReLU()\n",
    "        self.conv2 = relax.frontend.nn.Conv2D(32, 64, kernel_size=5, stride=2, padding=2, bias=True)\n",
    "        self.relu2 = relax.frontend.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 128, 128)\n",
    "mod, params = MyRelaxModel().export_tvm({\"forward\": {\"x\": relax.frontend.nn.spec.Tensor(input_shape, \"float32\")}})\n",
    "\n",
    "# Unlike in torch, model parameters are not initialized automatically. Let's create them by hand\n",
    "# We'll need them at profile time\n",
    "model_params_tvm = [tvm.nd.array(np.random.randn(*x[1].shape).astype(np.float32)) for x in params]\n",
    "model_params_tvm_labels = [x[0] for x in params]\n",
    "\n",
    "mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "class IRDiffer:\n",
    "    def __init__(self, init_state=None):\n",
    "        self.last_state = None\n",
    "        self.current_state = None\n",
    "        if init_state is not None:\n",
    "            self.update(init_state)\n",
    "\n",
    "    def update(self, script):\n",
    "        if isinstance(script, tvm.ir.IRModule):\n",
    "            script = script.script()\n",
    "        elif isinstance(script, str):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot parse type {type(script)}\")\n",
    "\n",
    "        self.last_state = self.current_state\n",
    "        self.current_state = script\n",
    "\n",
    "    def show_diff(self) -> None:\n",
    "        \"\"\"\n",
    "        Prints the git-like diff between two relax IR states.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.last_state, str) and isinstance(self.current_state, str):\n",
    "            raise ValueError(f\"Can only compare two str, not {type(self.last_state)} and {type(self.current_state)}\")\n",
    "        line_color = {\"+\": 32, \"-\": 31}\n",
    "\n",
    "        a = self.last_state\n",
    "        b = self.current_state\n",
    "        diffs = difflib.ndiff(a.splitlines(keepends=True), b.splitlines(keepends=True))\n",
    "        diff_list = list(diffs)\n",
    "        styled: list[str] = []\n",
    "        for prev, next in zip(diff_list, diff_list[1:] + [\"\"]):\n",
    "            color = line_color.get(prev[0], 0)\n",
    "            match prev[0]:\n",
    "                case \" \":\n",
    "                    styled.append(prev)\n",
    "                case \"+\" | \"-\":\n",
    "                    index = [i for i, c in enumerate(next) if c == \"^\"]\n",
    "                    _prev = list(prev)\n",
    "                    for idx in index:\n",
    "                        _prev[idx] = f\"\\x1b[97;{color+10};1m{_prev[idx]}\\x1b[0;{color}m\"\n",
    "                    styled.append(f'\\x1b[{color}m{\"\".join(_prev)}\\x1b[0m')\n",
    "                case \"?\":\n",
    "                    continue\n",
    "        print(\"\".join(styled))\n",
    "\n",
    "irdiffer = IRDiffer(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # from tvm.script import ir as I\n",
      "\u001b[32m+ # from tvm.script import tir as T\n",
      "\u001b[0m  # from tvm.script import relax as R\n",
      "  \n",
      "  @I.ir_module\n",
      "  class Module:\n",
      "\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def add(lv: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\"), lv1: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), \"float32\"), T_add: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(64), T.int64(64)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"T_add\"):\n",
      "\u001b[0m\u001b[32m+                 v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(lv[v_ax0, v_ax1, v_ax2, v_ax3], lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])\n",
      "\u001b[0m\u001b[32m+                 T_add[v_ax0, v_ax1, v_ax2, v_ax3] = lv[v_ax0, v_ax1, v_ax2, v_ax3] + lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def add1(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(32), T.int64(32)), \"float32\"), lv3: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), \"float32\"), T_add: T.Buffer((T.int64(1), T.int64(64), T.int64(32), T.int64(32)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(32), T.int64(32)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"T_add\"):\n",
      "\u001b[0m\u001b[32m+                 v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(lv2[v_ax0, v_ax1, v_ax2, v_ax3], lv3[v_ax0, v_ax1, T.int64(0), T.int64(0)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])\n",
      "\u001b[0m\u001b[32m+                 T_add[v_ax0, v_ax1, v_ax2, v_ax3] = lv2[v_ax0, v_ax1, v_ax2, v_ax3] + lv3[v_ax0, v_ax1, T.int64(0), T.int64(0)]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def conv2d(x: T.Buffer((T.int64(1), T.int64(3), T.int64(128), T.int64(128)), \"float32\"), conv1_weight: T.Buffer((T.int64(32), T.int64(3), T.int64(5), T.int64(5)), \"float32\"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         pad_temp = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(132), T.int64(132)))\n",
      "\u001b[0m\u001b[32m+         for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(3), T.int64(132), T.int64(132)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"pad_temp\"):\n",
      "\u001b[0m\u001b[32m+                 v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(x[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(2) <= v_i2 and v_i2 < T.int64(130) and T.int64(2) <= v_i3 and v_i3 < T.int64(130), x[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)], T.float32(0.0))\n",
      "\u001b[0m\u001b[32m+         for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(32), T.int64(64), T.int64(64), T.int64(3), T.int64(5), T.int64(5)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"conv2d_nchw\"):\n",
      "\u001b[0m\u001b[32m+                 v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap(\"SSSSRRR\", [nn, ff, yy, xx, rc, ry, rx])\n",
      "\u001b[0m\u001b[32m+                 T.reads(pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], conv1_weight[v_ff, v_rc, v_ry, v_rx])\n",
      "\u001b[0m\u001b[32m+                 T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])\n",
      "\u001b[0m\u001b[32m+                 with T.init():\n",
      "\u001b[0m\u001b[32m+                     conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0.0)\n",
      "\u001b[0m\u001b[32m+                 conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * conv1_weight[v_ff, v_rc, v_ry, v_rx]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def conv2d1(relu: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\"), conv2_weight: T.Buffer((T.int64(64), T.int64(32), T.int64(5), T.int64(5)), \"float32\"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(64), T.int64(32), T.int64(32)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         pad_temp = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(68), T.int64(68)))\n",
      "\u001b[0m\u001b[32m+         for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(32), T.int64(68), T.int64(68)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"pad_temp\"):\n",
      "\u001b[0m\u001b[32m+                 v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(relu[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(2) <= v_i2 and v_i2 < T.int64(66) and T.int64(2) <= v_i3 and v_i3 < T.int64(66), relu[v_i0, v_i1, v_i2 - T.int64(2), v_i3 - T.int64(2)], T.float32(0.0))\n",
      "\u001b[0m\u001b[32m+         for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(64), T.int64(32), T.int64(32), T.int64(32), T.int64(5), T.int64(5)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"conv2d_nchw\"):\n",
      "\u001b[0m\u001b[32m+                 v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap(\"SSSSRRR\", [nn, ff, yy, xx, rc, ry, rx])\n",
      "\u001b[0m\u001b[32m+                 T.reads(pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], conv2_weight[v_ff, v_rc, v_ry, v_rx])\n",
      "\u001b[0m\u001b[32m+                 T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])\n",
      "\u001b[0m\u001b[32m+                 with T.init():\n",
      "\u001b[0m\u001b[32m+                     conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0.0)\n",
      "\u001b[0m\u001b[32m+                 conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * conv2_weight[v_ff, v_rc, v_ry, v_rx]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def relu(conv2d: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\"), compute: T.Buffer((T.int64(1), T.int64(32), T.int64(64), T.int64(64)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(32), T.int64(64), T.int64(64)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"compute\"):\n",
      "\u001b[0m\u001b[32m+                 v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(conv2d[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 T.writes(compute[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 compute[v_i0, v_i1, v_i2, v_i3] = T.max(conv2d[v_i0, v_i1, v_i2, v_i3], T.float32(0.0))\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def relu1(conv2d1: T.Buffer((T.int64(1), T.int64(64), T.int64(32), T.int64(32)), \"float32\"), compute: T.Buffer((T.int64(1), T.int64(64), T.int64(32), T.int64(32)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(64), T.int64(32), T.int64(32)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"compute\"):\n",
      "\u001b[0m\u001b[32m+                 v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(conv2d1[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 T.writes(compute[v_i0, v_i1, v_i2, v_i3])\n",
      "\u001b[0m\u001b[32m+                 compute[v_i0, v_i1, v_i2, v_i3] = T.max(conv2d1[v_i0, v_i1, v_i2, v_i3], T.float32(0.0))\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def reshape(conv1_bias: T.Buffer((T.int64(32),), \"float32\"), T_reshape: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"T_reshape\"):\n",
      "\u001b[0m\u001b[32m+                 v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(conv1_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(32)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
      "\u001b[0m\u001b[32m+                 T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = conv1_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(32)]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m\u001b[32m+     @T.prim_func(private=True)\n",
      "\u001b[0m\u001b[32m+     def reshape1(conv2_bias: T.Buffer((T.int64(64),), \"float32\"), T_reshape: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), \"float32\")):\n",
      "\u001b[0m\u001b[32m+         T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "\u001b[0m\u001b[32m+         # with T.block(\"root\"):\n",
      "\u001b[0m\u001b[32m+         for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(1), T.int64(1)):\n",
      "\u001b[0m\u001b[32m+             with T.block(\"T_reshape\"):\n",
      "\u001b[0m\u001b[32m+                 v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap(\"SSSS\", [ax0, ax1, ax2, ax3])\n",
      "\u001b[0m\u001b[32m+                 T.reads(conv2_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(64)])\n",
      "\u001b[0m\u001b[32m+                 T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
      "\u001b[0m\u001b[32m+                 T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = conv2_bias[(v_ax1 + v_ax2 + v_ax3) % T.int64(64)]\n",
      "\u001b[0m\u001b[32m+ \n",
      "\u001b[0m      @R.function\n",
      "      def forward(x: R.Tensor((1, 3, 128, 128), dtype=\"float32\"), conv1_weight: R.Tensor((32, 3, 5, 5), dtype=\"float32\"), conv1_bias: R.Tensor((32,), dtype=\"float32\"), conv2_weight: R.Tensor((64, 32, 5, 5), dtype=\"float32\"), conv2_bias: R.Tensor((64,), dtype=\"float32\")) -> R.Tensor((1, 64, 32, 32), dtype=\"float32\"):\n",
      "          R.func_attr({\"num_input\": 1})\n",
      "\u001b[32m+         cls = Module\n",
      "\u001b[0m          with R.dataflow():\n",
      "\u001b[31m-             lv: R.Tensor((1, 32, 64, 64), dtype=\"float32\") = R.nn.conv2d(x, conv1_weight, strides=[2, 2], padding=[2, 2, 2, 2], dilation=[1, 1], groups=1, data_layout=\"NCHW\", kernel_layout=\"OIHW\", out_layout=\"NCHW\", out_dtype=\"void\")\n",
      "\u001b[0m\u001b[31m-             lv1: R.Tensor((1, 32, 1, 1), dtype=\"float32\") = R.reshape(conv1_bias, R.shape([1, 32, 1, 1]))\n",
      "\u001b[0m\u001b[31m-             conv2d: R.Tensor((1, 32, 64, 64), dtype=\"float32\") = R.add(lv, lv1)\n",
      "\u001b[0m\u001b[31m-             relu: R.Tensor((1, 32, 64, 64), dtype=\"float32\") = R.nn.relu(conv2d)\n",
      "\u001b[0m\u001b[31m-             lv2: R.Tensor((1, 64, 32, 32), dtype=\"float32\") = R.nn.conv2d(relu, conv2_weight, strides=[2, 2], padding=[2, 2, 2, 2], dilation=[1, 1], groups=1, data_layout=\"NCHW\", kernel_layout=\"OIHW\", out_layout=\"NCHW\", out_dtype=\"void\")\n",
      "\u001b[0m\u001b[31m-             lv3: R.Tensor((1, 64, 1, 1), dtype=\"float32\") = R.reshape(conv2_bias, R.shape([1, 64, 1, 1]))\n",
      "\u001b[0m\u001b[31m-             conv2d1: R.Tensor((1, 64, 32, 32), dtype=\"float32\") = R.add(lv2, lv3)\n",
      "\u001b[0m\u001b[31m-             relu1: R.Tensor((1, 64, 32, 32), dtype=\"float32\") = R.nn.relu(conv2d1)\n",
      "\u001b[0m\u001b[32m+             lv = R.call_tir(cls.conv2d, (x, conv1_weight), out_sinfo=R.Tensor((1, 32, 64, 64), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             lv1 = R.call_tir(cls.reshape, (conv1_bias,), out_sinfo=R.Tensor((1, 32, 1, 1), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             conv2d = R.call_tir(cls.add, (lv, lv1), out_sinfo=R.Tensor((1, 32, 64, 64), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             relu = R.call_tir(cls.relu, (conv2d,), out_sinfo=R.Tensor((1, 32, 64, 64), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             lv2 = R.call_tir(cls.conv2d1, (relu, conv2_weight), out_sinfo=R.Tensor((1, 64, 32, 32), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             lv3 = R.call_tir(cls.reshape1, (conv2_bias,), out_sinfo=R.Tensor((1, 64, 1, 1), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             conv2d1 = R.call_tir(cls.add1, (lv2, lv3), out_sinfo=R.Tensor((1, 64, 32, 32), dtype=\"float32\"))\n",
      "\u001b[0m\u001b[32m+             relu1 = R.call_tir(cls.relu1, (conv2d1,), out_sinfo=R.Tensor((1, 64, 32, 32), dtype=\"float32\"))\n",
      "\u001b[0m              gv: R.Tensor((1, 64, 32, 32), dtype=\"float32\") = relu1\n",
      "              R.output(gv)\n",
      "          return gv\n"
     ]
    }
   ],
   "source": [
    "# https://tvm.apache.org/docs/reference/api/python/transform.html\n",
    "transforms = [\n",
    "    # Phase 1. Passes on high-level operator graph\n",
    "    # There's not much we can do here with this model we defined\n",
    "    # relax.transform.FuseTransposeMatmul(),\n",
    "\n",
    "    # Phase 2. Lowering to TIR, inherited TVM Relax's official \"zero\" pipeline\n",
    "    relax.transform.LegalizeOps(),\n",
    "    # relax.transform.AnnotateTIROpPattern(),\n",
    "    # relax.transform.FoldConstant(),\n",
    "    # relax.transform.ConvertToDataflow(),\n",
    "    # relax.transform.FuseOps(),\n",
    "    # relax.transform.FuseTIR(),\n",
    "\n",
    "    # Phase 3. Passes on TIR\n",
    "    # relax.transform.DeadCodeElimination(),\n",
    "\n",
    "    # Phase 4. Lowering to VM bytecode\n",
    "    # relax.transform.RewriteDataflowReshape(),\n",
    "    # relax.transform.ToNonDataflow(),\n",
    "    # relax.transform.RemovePurityChecking(),\n",
    "    # relax.transform.CallTIRRewrite(),\n",
    "    # relax.transform.StaticPlanBlockMemory(),\n",
    "    # relax.transform.RewriteCUDAGraph(),\n",
    "    # relax.transform.LowerAllocTensor(),\n",
    "    # relax.transform.KillAfterLastUse(),\n",
    "    # relax.transform.LowerRuntimeBuiltin(),\n",
    "    # relax.transform.VMShapeLower(),\n",
    "    # relax.transform.AttachGlobalSymbol(),\n",
    "]\n",
    "\n",
    "\n",
    "new_mod = mod\n",
    "for t in transforms:\n",
    "    new_mod = t(new_mod)\n",
    "irdiffer.update(new_mod)\n",
    "\n",
    "try:\n",
    "    irdiffer.show_diff()\n",
    "except ValueError as e:\n",
    "    print(f\"failed, maybe uninitialized irdiffer. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), lv1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_ax0, v_ax1, v_ax2, v_ax3], lv1[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> lv[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">+</span> lv1[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add1</span>(lv2: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), lv3: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv2[v_ax0, v_ax1, v_ax2, v_ax3], lv3[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_add[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> lv2[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">+</span> lv3[v_ax0, v_ax1, T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">conv2d</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2d_nchw: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        pad_temp <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">132</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>if_then_else(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i2 <span style=\"color: #008000; font-weight: bold\">and</span> v_i2 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>) <span style=\"color: #008000; font-weight: bold\">and</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i3 <span style=\"color: #008000; font-weight: bold\">and</span> v_i3 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">130</span>), x[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> nn, ff, yy, xx, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx], conv1_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">+</span> pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx] <span style=\"color: #A2F; font-weight: bold\">*</span> conv1_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">conv2d1</span>(relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2d_nchw: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        pad_temp <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">68</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">68</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">68</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">68</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_temp&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(relu[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(pad_temp[v_i0, v_i1, v_i2, v_i3])\n",
       "                pad_temp[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>if_then_else(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i2 <span style=\"color: #008000; font-weight: bold\">and</span> v_i2 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">66</span>) <span style=\"color: #008000; font-weight: bold\">and</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">&lt;=</span> v_i3 <span style=\"color: #008000; font-weight: bold\">and</span> v_i3 <span style=\"color: #A2F; font-weight: bold\">&lt;</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">66</span>), relu[v_i0, v_i1, v_i2 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), v_i3 <span style=\"color: #A2F; font-weight: bold\">-</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> nn, ff, yy, xx, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv2d_nchw&quot;</span>):\n",
       "                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [nn, ff, yy, xx, rc, ry, rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx], conv2_weight[v_ff, v_rc, v_ry, v_rx])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2d_nchw[v_nn, v_ff, v_yy, v_xx] <span style=\"color: #A2F; font-weight: bold\">+</span> pad_temp[v_nn, v_rc, v_yy <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_ry, v_xx <span style=\"color: #A2F; font-weight: bold\">*</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #A2F; font-weight: bold\">+</span> v_rx] <span style=\"color: #A2F; font-weight: bold\">*</span> conv2_weight[v_ff, v_rc, v_ry, v_rx]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">relu</span>(conv2d: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2d[v_i0, v_i1, v_i2, v_i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(conv2d[v_i0, v_i1, v_i2, v_i3], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">relu1</span>(conv2d1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2, i3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1, v_i2, v_i3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [i0, i1, i2, i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2d1[v_i0, v_i1, v_i2, v_i3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute[v_i0, v_i1, v_i2, v_i3])\n",
       "                compute[v_i0, v_i1, v_i2, v_i3] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(conv2d1[v_i0, v_i1, v_i2, v_i3], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">reshape</span>(conv1_bias: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_reshape: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_reshape&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv1_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv1_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">reshape1</span>(conv2_bias: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_reshape: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2, ax3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_reshape&quot;</span>):\n",
       "                v_ax0, v_ax1, v_ax2, v_ax3 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [ax0, ax1, ax2, ax3])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(conv2_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])\n",
       "                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] <span style=\"color: #A2F; font-weight: bold\">=</span> conv2_bias[(v_ax1 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax2 <span style=\"color: #A2F; font-weight: bold\">+</span> v_ax3) <span style=\"color: #A2F; font-weight: bold\">%</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">64</span>)]\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">32</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">5</span>, <span style=\"color: #008000\">5</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d, (x, conv1_weight), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>reshape, (conv1_bias,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            conv2d <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add, (lv, lv1), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            relu <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>relu, (conv2d,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d1, (relu, conv2_weight), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>reshape1, (conv2_bias,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            conv2d1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add1, (lv2, lv3), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            relu1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>relu1, (conv2d1,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> relu1\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling the optimized module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to activate the lowering passes in your transforms define after \"phase 4\"\n",
    "\n",
    "    # relax.transform.ToNonDataflow(),\n",
    "    # relax.transform.RemovePurityChecking(),\n",
    "    # relax.transform.CallTIRRewrite(),\n",
    "    # relax.transform.StaticPlanBlockMemory(),\n",
    "    # relax.transform.RewriteCUDAGraph(),\n",
    "    # relax.transform.LowerAllocTensor(),\n",
    "    # relax.transform.KillAfterLastUse(),\n",
    "    # relax.transform.LowerRuntimeBuiltin(),\n",
    "    # relax.transform.VMShapeLower(),\n",
    "    # relax.transform.AttachGlobalSymbol(),\n",
    "\n",
    "else it doesn't really know how to lower the IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tvm.nd.array(np.random.randn(1, 3, 128, 128).astype(np.float32))\n",
    "\n",
    "# If you change the pipeline, change the name !\n",
    "pipeline_name = \"opt_test\"\n",
    "try:\n",
    "    tvm.relax.get_pipeline(pipeline_name)\n",
    "    print(f\"Using previously defined pipeline\")\n",
    "except:\n",
    "    @tvm.relax.register_pipeline(pipeline_name)\n",
    "    def _pipeline(  # pylint: disable=too-many-arguments\n",
    "        ext_mods: typing.List[tvm.relax.frontend.nn.ExternModule] = None,\n",
    "    ):\n",
    "        ext_mods = ext_mods or []\n",
    "\n",
    "        @tvm.transform.module_pass(opt_level=1)\n",
    "        def _pipeline(mod: tvm.ir.IRModule, _ctx: tvm.transform.PassContext) -> tvm.ir.IRModule:\n",
    "            seq = tvm.transform.Sequential(\n",
    "                transforms  # <-- Defined in previous cells /!\\ you are applying these transforms\n",
    "            )\n",
    "            mod = seq(mod)\n",
    "            return mod\n",
    "\n",
    "        return _pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  6: _ZN3tvm7runtime13PackedFuncObj\n  5: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::relax::ExecBuilder, tvm::IRModule)>::AssignTypedLambda<tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule)>(tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  4: tvm::relax::relax_vm::VMCodeGen(tvm::relax::ExecBuilder, tvm::IRModule)\n  3: tvm::relax::relax_vm::CodeGenVM::Run(tvm::relax::ExecBuilder, tvm::IRModule)\n  2: tvm::relax::relax_vm::CodeGenVM::Codegen(tvm::relax::Function const&)\n  1: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::SeqExprNode const*)\n  0: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::CallNode const*)\n  File \"/home/arthur/Desktop/Projects/fromsource/d2l-tvm/tvm/src/relax/backend/vm/codegen_vm.cc\", line 157\nTVMError: CodeGenVM cannot handle this intrinsic now:\nOp(relax.call_tir)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ex \u001b[38;5;241m=\u001b[39m \u001b[43mrelax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllvm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can try 'default_build'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vm \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mVirtualMachine(ex, tvm\u001b[38;5;241m.\u001b[39mcpu(), profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m vm\u001b[38;5;241m.\u001b[39mtime_evaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, dev\u001b[38;5;241m=\u001b[39mtvm\u001b[38;5;241m.\u001b[39mcpu(), min_repeat_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)(\n\u001b[1;32m      4\u001b[0m     dummy_input, \u001b[38;5;241m*\u001b[39mmodel_params_tvm\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/relax/vm_build.py:352\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, pipeline, exec_mode, system_lib)\u001b[0m\n\u001b[1;32m    350\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(constants))\n\u001b[1;32m    351\u001b[0m builder \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mExecBuilder()\n\u001b[0;32m--> 352\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43m_vmcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexec_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _vmlink(\n\u001b[1;32m    354\u001b[0m     builder\u001b[38;5;241m=\u001b[39mbuilder,\n\u001b[1;32m    355\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m     system_lib\u001b[38;5;241m=\u001b[39msystem_lib,\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/relax/vm_build.py:176\u001b[0m, in \u001b[0;36m_vmcodegen\u001b[0;34m(builder, mod, exec_mode)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Running VM codegen.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    Left over IRModule that may contain extra functions.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exec_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytecode\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ffi_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVMCodeGen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exec_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ffi_api\u001b[38;5;241m.\u001b[39mVMTIRCodeGen(builder, mod)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/_ffi/_cython/packed_func.pxi:339\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/_ffi/_cython/packed_func.pxi:270\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/_ffi/_cython/packed_func.pxi:259\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/_ffi/_cython/base.pxi:185\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Projects/fromsource/d2l-tvm/tvm/python/tvm/_ffi/base.py:481\u001b[0m, in \u001b[0;36mraise_last_ffi_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# The exception PyObject may contain a large amount of state,\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# including all stack frames that may be inspected in a later\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# PDB post-mortem.  Therefore, we must make sure to remove the\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# underlying PyObject* from the C++ side after we retrieve it.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m _LIB\u001b[38;5;241m.\u001b[39mTVMDropLastPythonError()\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m py_err\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  6: _ZN3tvm7runtime13PackedFuncObj\n  5: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::relax::ExecBuilder, tvm::IRModule)>::AssignTypedLambda<tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule)>(tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  4: tvm::relax::relax_vm::VMCodeGen(tvm::relax::ExecBuilder, tvm::IRModule)\n  3: tvm::relax::relax_vm::CodeGenVM::Run(tvm::relax::ExecBuilder, tvm::IRModule)\n  2: tvm::relax::relax_vm::CodeGenVM::Codegen(tvm::relax::Function const&)\n  1: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::SeqExprNode const*)\n  0: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::CallNode const*)\n  File \"/home/arthur/Desktop/Projects/fromsource/d2l-tvm/tvm/src/relax/backend/vm/codegen_vm.cc\", line 157\nTVMError: CodeGenVM cannot handle this intrinsic now:\nOp(relax.call_tir)"
     ]
    }
   ],
   "source": [
    "ex = relax.build(new_mod, target=\"llvm\", pipeline=relax.get_pipeline(pipeline_name))  # You can try 'default_build'\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu(), profile=True)\n",
    "evaluator = vm.time_evaluator(\"forward\", dev=tvm.cpu(), min_repeat_ms=200)(\n",
    "    dummy_input, *model_params_tvm\n",
    ")\n",
    "evaluator  # If error = TVMError: CodeGenVM cannot handle this intrinsic now: Op(relax.call_tir) --> apply phase 4 lowerings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
